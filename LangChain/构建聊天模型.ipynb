{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b01cd7",
   "metadata": {},
   "source": [
    "### 环境要求\n",
    "\n",
    "python 3.12 与最新 langchain 存在兼容问题，所以降级安装3.9版本\n",
    "\n",
    "```cmd\n",
    "conda create -n .venv python=3.9\n",
    "conda activate .venv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f38cf4",
   "metadata": {},
   "source": [
    "### 安装依赖\n",
    "```cmd\n",
    "conda install ipykernel  // jupyter notebook 需要\n",
    "conda install python-dotenv -c conda-forge \n",
    "conda install langchain -c conda-forge \n",
    "conda install langchain-community -c conda-forge\n",
    "conda install openai -c conda-forge\n",
    "conda install tiktoken -c conda-forge  // 对话字符缓存使用\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734ec57",
   "metadata": {},
   "source": [
    "### 通过 LangChain 使用 OpenAI\n",
    "三个重要的概念：模型、提示与解释器\n",
    "#### 模型\n",
    "从 `langchain.chat_models`导入`OpenAI`的对话模型`ChatOpenAI`，除了`OpenAI` 以外，`chat_models`也集成了其它的对话模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "# 获取环境变量\n",
    "api_key = os.environ[\"OPENAI_API_KEY_LOCAL\"]\n",
    "base_url = os.environ[\"OPENAI_BASE_URL\"]\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "chat = ChatOpenAI(temperature=0.0, api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72fb86",
   "metadata": {},
   "source": [
    "#### 提示模板\n",
    "在应用于比较复杂的场景时，提示可能会非常长并且包含涉及许多细节。使用提示模版，可以让我们更为方便地重复使用设计好的提示。\n",
    "LangChain还提供了提示模版用于一些常用场景。比如自动摘要、问答、连接到SQL数据库、连接到不同的API。通过使用LangChain内置的提示模版，你可以快速建立自己的大模型应用，而不需要花时间去设计和构造提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写 prompt\n",
    "template_string = \"\"\"把由三个反引号分隔的文本\\\n",
    "翻译成一种{style}风格。\\\n",
    "文本: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "# 然后，我们调用`ChatPromptTemplatee.from_template()`函数将\n",
    "# 上面的提示模版字符`template_string`转换为提示模版`prompt_template`\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "customer_style = \"\"\"正式普通话 \\\n",
    "用一个平静、尊敬的语气\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "有人违法使用我的电话信息，请马上注销，不然我将投诉12315/信息安全局以及曝光，请2小时马上答复我。\n",
    "\"\"\"\n",
    "\n",
    "# 使用提示模版\n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style, text=customer_email\n",
    ")\n",
    "\n",
    "customer_response = chat(customer_messages)\n",
    "print(customer_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05fc2b",
   "metadata": {},
   "source": [
    "#### 输出解释器\n",
    "以上`customer_response`输出为 `str` 类型，如果想要方便的提取信息，需要使用`Langchain`中的输出解释器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "template_string = \"\"\"\n",
    "对于以下文本，提取出以下信息：\n",
    "\n",
    "语气：语气是否平和\n",
    "\n",
    "语言风格：语言风格是否正式\n",
    "\n",
    "使用以下键将输出格式化为 JSON：\n",
    "语气\n",
    "语言风格\n",
    "\n",
    "\n",
    "文本：{text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "response_schemas  = [\n",
    "  ResponseSchema(name=\"语气\", description=\"语气是否平和\"),\n",
    "  ResponseSchema(name=\"语言风格\", description=\"语言风格是否正式\")\n",
    "]\n",
    "\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(\"输出格式规定：\",format_instructions)\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "有人违法使用我的电话信息，请马上注销，不然我将投诉12315/信息安全局以及曝光，请2小时马上答复我。\n",
    "\"\"\"\n",
    "\n",
    "# 使用提示模版\n",
    "message = prompt_template.format_messages(text=text, format_instructions=format_instructions)\n",
    "\n",
    "customer_response = chat(message)\n",
    "print(\"结果类型:\", type(customer_response.content))\n",
    "print(customer_response.content)\n",
    "\n",
    "# output_dict = output_parser.parse(customer_response.content)\n",
    "# print(\"解析后的结果类型:\", type(output_dict))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd5af1",
   "metadata": {},
   "source": [
    "### 储存 Memory\n",
    "使用 LangChain 中的储存(Memory)模块时，它旨在保存、组织和跟踪整个对话的历史，从而为用户和模型之间的交互提供连续的上下文。\n",
    "\n",
    "LangChain 提供了多种储存类型。其中:\n",
    "* 缓冲区储存允许保留最近的聊天消息\n",
    "* 摘要储存则提供了对整个对话的摘要。\n",
    "* 实体储存则允许在多轮对话中保留有关特定实体的信息。\n",
    "\n",
    "这些记忆组件都是模块化的，可与其他组件组合使用，从而增强机器人的对话管理能力。\n",
    "\n",
    "主要的 4 种存储模块：\n",
    "* 对话缓存储存 (ConversationBufferMemory）\n",
    "* 对话缓存窗口储存 (ConversationBufferWindowMemory）\n",
    "* 对话令牌缓存储存 (ConversationTokenBufferMemory）\n",
    "* 对话摘要缓存储存 (ConversationSummaryBufferMemory）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972d5e4",
   "metadata": {},
   "source": [
    "#### 对话缓存存储\n",
    "属于缓冲区储存器，用于存储对话上下文信息，包括历史消息、当前消息、历史消息的embedding等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02039041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试增大该参数。\n",
    "llm = ChatOpenAI(temperature=0.0, api_key=api_key, base_url=base_url)\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "\n",
    "# 新建一个 ConversationChain Class 实例\n",
    "# verbose参数设置为True时，程序会输出更详细的信息，以提供更多的调试或运行时信息。\n",
    "# 相反，当将verbose参数设置为False时，程序会以更简洁的方式运行，只输出关键的信息。\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True )\n",
    "\n",
    "conversation.predict(input=\"你好, 我叫池\")\n",
    "conversation.predict(input=\"1+1 等于几\")\n",
    "conversation.predict(input=\"我叫什么\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9759e",
   "metadata": {},
   "source": [
    "#### 对话窗口存储\n",
    "对话缓存窗口储存只保留一个窗口大小的对话。它只使用最近的n次交互。这可以用于保持最近交互的滑动窗口，以便缓冲区不会过大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e377104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一轮对话：\n",
      "哦，原来你也叫池啊！真巧！请问有什么问题或者话题想和我聊吗？我可以告诉你关于天气、新闻、历史等各种信息哦。\n",
      "第二轮对话：\n",
      "抱歉，我无法知道你的名字。你可以告诉我你的名字，这样我就能记住了！有什么其他问题或者话题想和我聊吗？\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k=1表明只保留一个对话记忆\n",
    "memory = ConversationBufferWindowMemory(k=1)  \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, api_key=api_key, base_url=base_url)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False  )\n",
    "\n",
    "print(\"第一轮对话：\")\n",
    "print(conversation.predict(input=\"你好, 我叫池\"))\n",
    "\n",
    "print(\"第二轮对话：\")\n",
    "print(conversation.predict(input=\"我叫什么名字？\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766adb2",
   "metadata": {},
   "source": [
    "#### 对话字符缓存存储\n",
    "使用对话字符缓存记忆，内存将限制保存的token数量。如果字符数量超出指定数目，它会切掉这个对话的早期部分 以保留与最近的交流相对应的字符数量，但不超过字符限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e64698a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: 轻舟已过万重山。'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"朝辞白帝彩云间，\"}, {\"output\": \"千里江陵一日还。\"})\n",
    "memory.save_context({\"input\": \"两岸猿声啼不住，\"}, {\"output\": \"轻舟已过万重山。\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c031801",
   "metadata": {},
   "source": [
    "### 模型链 Chains\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
